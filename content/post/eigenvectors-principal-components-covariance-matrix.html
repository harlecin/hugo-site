---
title: "PCA, Eigenvectors and the Covariance Matrix"
author: "Christoph"
date: 2020-05-27T14:41:14-05:00
categories: ["theory"]
tags: ["math"]
---



<p>Almost every data science course will at some point (usually) sooner than later cover PCA, i.e. Principal Component Analysis. PCA is an important tool used in exploratory data analysis for dimensionality reduction. In this post I want
to show you (hopefully in an intuitive way) how PCA works its mathematical magic.</p>
<p>Let’s start with a short description of PCA taken straight from <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Wikipedia</a>:</p>
<blockquote>
<p>PCA is "a statistical procedure that uses an an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component […] accounts for as much of the variability in the data as possible, and each succedding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.</p>
</blockquote>
<p>So, what does that mean exactly?</p>
<p>Let’s start by looking at a very simple example and take it from there:</p>
<pre class="r"><code>library(ggplot2)
library(data.table)

ggplot(mtcars, aes(x = scale(wt), y = scale(mpg))) + 
  geom_point() +
  labs(title=&quot;Relationship between mpg and weight&quot;, 
       subtitle=&quot;Standardized variables (mean = 0, std = 1)&quot;) +
  geom_smooth(method = &quot;lm&quot;, se = FALSE) +
  coord_fixed(ratio = 1, xlim = NULL, ylim = NULL, clip = &quot;on&quot;)+
  geom_hline(yintercept = 0, color = &quot;black&quot;) +
  geom_vline(xintercept = 0, color = &quot;black&quot;)</code></pre>
<p><img src="/post/eigenvectors-principal-components-covariance-matrix_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>We can clearly see the linear relationship between weight (wt) and miles per gallon (mpg). Most of the points fit rather tightly around the blue line, without deviating to much above or below. So, if we had to summarise our data with one dimension only, it would be quite reasonable to pick a dimension somewhat similar to the blue line. This corresponds to a change in basis. We are rotating our standard basis given by the vectors (1,0) and (0,1) to hopefully find a new basis that better captures the dynamics in our data.</p>
<p>In the next plot, I added one possible new coordinate system (red) that we could use: I set the axis to be equal to the eigenvectors of the covariance matrix of our dataset.</p>
<p><img src="/post/eigenvectors-principal-components-covariance-matrix_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>This basis clearly captures the dynamics of our data better than the original one did, but why choose this particular one and not e.g. a coordinate system based on our blue regression line?</p>
<p>Before we continue, let’s take a look at the covariance matrix of mpg and wt.</p>
<p>Remember, a covariance matrix looks like this:
<span class="math display">\[
Cov(x, y) = 
            \begin{bmatrix}
                   \sigma_x^2 &amp; \sigma_{x,y} \\
                   \sigma_{y, x} &amp; \sigma_y^2
             \end{bmatrix}
\]</span>
where <span class="math inline">\(\sigma_{x,y}=\sigma_{y,x}\)</span>.</p>
<p>We can easily calculate it using R’s <code>cov</code> function:</p>
<pre class="r"><code>cov(mt)</code></pre>
<pre><code>##            [,1]       [,2]
## [1,]  1.0000000 -0.8676594
## [2,] -0.8676594  1.0000000</code></pre>
<p>The following exposition is based on the excellent overview of how PCA works by Jon Shlens.</p>
<p>We have already seen in the picture that mpg depends on the weight of the car (the variables covary) which is why the covariance matrix has off-diagonal elements that are not zero. Now imagine we could rotate our coordinate axis so that in the new coordinate system the covariance matrix looks like this:
<span class="math display">\[
Cov(x, y) = 
            \begin{bmatrix}
                   \sigma_x^2 &amp; 0 \\
                   0 &amp; \sigma_y^2
             \end{bmatrix}
\]</span>
The two variables are uncorrelate given our new basis and the axis are aligned with the direction of largest variance so we can easily see which axis is most important to explain the variance in our data.</p>
<p>So given our matrix of observations <span class="math inline">\(X\)</span>, we are looking for a linear transformation matrix <span class="math inline">\(P\)</span> such that we get a new matrix of observations <span class="math inline">\(Y\)</span> with a covariance matrix that is diagonal like above:</p>
<p><span class="math display">\[
Y = PX \text{ with } \text{Cov}_Y \text{ diagonal}
\]</span></p>
<p>Let’s start by writing down the matrix expression of the covariance matrix (you might want to take a look at my previous blog post where I summarised some matrix calculation rules):
<span class="math display">\[
\begin{align}
\text{Cov}_Y &amp;= \frac{1}{n-1}YY^t\\
 &amp; = \frac{1}{n-1}(PX)(PX)^t \\
 &amp; = \frac{1}{n-1} PXX^tP^t \\
 &amp; = \frac{1}{n-1} P(XX^t)P^t \\
 &amp; = \frac{1}{n-1} PAP^t
\end{align}
\]</span>
We get a new matrix <span class="math inline">\(A\)</span> which is symmetric by construction. Now we need to brush off our rusty linear algebra knowledge to remember that a symmetric matrix is diagonalized by an orthogonal matrix of its eigenvectors. So we get the following:</p>
<p><span class="math display">\[
A = EDE^t
\]</span></p>
<p>where E is a matrix of eigenvectors of A and D is a diagonal matrix. So if we want to diagonalize <span class="math inline">\(C_Y\)</span>, we need to pick a linear transformation <span class="math inline">\(P\)</span> = <span class="math inline">\(E^t\)</span> and use the fact that <span class="math inline">\(P^{-1}=P^t\)</span> for orthonormal matrices and we get:
<span class="math display">\[
\begin{align}
\text{Cov}_Y &amp;= \frac{1}{n-1}YY^t\\
 &amp; = \frac{1}{n-1} PAP^t \\
 &amp; = \frac{1}{n-1} PEDE^tP^t \\
 &amp; = \frac{1}{n-1} E^tEDE^tE \\
 &amp; = \frac{1}{n-1} D
\end{align}
\]</span>
Since we have:
<span class="math display">\[
P = E^t = E^{-1}
\]</span>
we see that the eigenvectors form a basis of our transformed space since:
<span class="math display">\[
Y = E^{-1}X
\]</span>
is the change of basis from the standard basis to the basis given by <span class="math inline">\(E\)</span>. Since a passive transformation has a corresponding active transformation, we can also view <span class="math inline">\(P\)</span> as a rotation and a stretch (see <a href="https://en.wikipedia.org/wiki/Active_and_passive_transformation#Active_transformation">Wikipedia</a> for more infos).</p>
<p>So we found the transformation that diagonalizes the covariance matrix <span class="math inline">\(\text{Cov}_Y\)</span> and that is in a nutshell, what PCA does. We can now select a subset of principal components to reduce the dimension of our dataset. In our particular case we get:</p>
<pre class="r"><code>mt = matrix(c(scale(mtcars$wt), scale(mtcars$mpg)), nrow = length(mtcars$mpg), ncol = 2)

eigen_vec = eigen(cov(mt))

eigen_vec</code></pre>
<pre><code>## eigen() decomposition
## $values
## [1] 1.8676594 0.1323406
## 
## $vectors
##            [,1]      [,2]
## [1,]  0.7071068 0.7071068
## [2,] -0.7071068 0.7071068</code></pre>
<p><strong>PS:</strong>
In case you were wondering, why the blue regression line did not correspond to the axis of the first prinicpal component: Regression tries to minimize the distance between the regression line and a point wrt to the y axis (<span class="math inline">\(\hat{y} - y\)</span>), whereas PCA tries to minimize the orthogonal distance. You can find a few nice graphics illustrating that point <a href="https://shankarmsy.github.io/posts/pca-vs-lr.html">here</a></p>
<p><strong>PPS:</strong>
Variance is affected by units of measurement, so it is neceassary to first standardize (z-transform) your data before applying PCA. Otherwise ‘large’ variables will naturally account for more variance and distort your analysis.</p>
<p><strong>PPPS:</strong>
Since PCA is an eigen-decomposition of the covariance matrix (which is mean-centered per definition) mean centering is not necessary, but singular-value-decomposition (svd), a commonly used alternative way to perform PCA, requires centered data. Check out the stackoverflow posts <a href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca">here</a> and <a href="https://stats.stackexchange.com/questions/189822/how-does-centering-make-a-difference-in-pca-for-svd-and-eigen-decomposition">here</a></p>
<div id="references" class="section level2">
<h2>References</h2>
<ul>
<li><a href="https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf">A Tutorial on Principal Component Analysis by Jon Shlens</a></li>
<li><a href="https://www.visiondummy.com/2014/04/geometric-interpretation-covariance-matrix/">A geometric interpretation of the covariance matrix</a></li>
<li><a href="https://www.youtube.com/watch?v=P2LTAUO1TdA&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&amp;index=13">Basis - 3Blue1Brown</a></li>
</ul>
</div>
