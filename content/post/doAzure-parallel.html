---
title: "Parallel processing in R using Azure Batch and Docker"
author: "Christoph"
date: 2018-03-05T21:13:14-05:00
categories: ["R"]
tags: ["machine learning", "R", "parallel processing", "Azure"]
---



<p>While (personal) computers have become increasingly powerful over the last years there are still lots of workloads that easily bring even the best workstation to its knees. Running huge Monte-Carlo simulations or training thousands of models takes hours, if not days even on very beefy machines. Now enter Azure Batch processing. Azure Batch is a Microsoft cloud computing service that allows you to run your computations on multiple machines in the cloud and collect the results back in R once the computations are done using <code>doAzureParallel</code> a <code>foreach</code> backend.</p>
<p>This article is intended as a quick start guide on how to use Azure Batch from R to distribute computations to the cloud and is based on Microsoft’s online documentation.</p>
<div id="prerequisites" class="section level2">
<h2>Prerequisites</h2>
<p>To start, you first need to install the following packages and load <code>doAzureParallel</code>:</p>
<pre class="r"><code># Install the devtools package  
install.packages(&quot;devtools&quot;) 

# Install rAzureBatch package
devtools::install_github(&quot;Azure/rAzureBatch&quot;) 

# Install the doAzureParallel package 
devtools::install_github(&quot;Azure/doAzureParallel&quot;) </code></pre>
<pre class="r"><code># Load the doAzureParallel library 
library(doAzureParallel) </code></pre>
<p>Next, you need to create an Azure Batch and a storage account in your Azure subscription. Jot down the access keys and save them in a json file with the following structure:</p>
<pre><code>{
  &quot;batchAccount&quot;: {
    &quot;name&quot;: &quot;your-batchaccount-name-here&quot;,
    &quot;key&quot;: &quot;your-key-here&quot;,
    &quot;url&quot;: &quot;your-url-here&quot;
  },
  &quot;storageAccount&quot;: {
    &quot;name&quot;: &quot;your-storageaccount-name-here&quot;,
    &quot;key&quot;: &quot;your-key-here&quot;
  },
  &quot;githubAuthenticationToken&quot;: &quot;&quot;
}</code></pre>
<p>Leave the github part blank. Now we need to load the credentials into our current R session:</p>
<pre class="r"><code>setCredentials(paste0(path_credentials, &quot;azure-batch-credentials.json&quot;))</code></pre>
<p>Then you need to specify the compute ressources Azure Batch is allowed to use. You can generate a configuration JSON in your current working directory using:</p>
<pre class="r"><code>generateClusterConfig(&quot;cluster.json&quot;)</code></pre>
<p>Per default, your batch includes 3 dedicated nodes and 3 low-priority nodes. Low-priority nodes are significantely cheaper, but may become unavailable if Azure needs additional capacity for other customers using normal nodes. I changed the configuration to include only low-priority nodes:</p>
<pre><code>{
  &quot;name&quot;: &quot;myTestPool&quot;,
  &quot;vmSize&quot;: &quot;Standard_D2_v2&quot;,
  &quot;maxTasksPerNode&quot;: 2,
  &quot;poolSize&quot;: {
    &quot;dedicatedNodes&quot;: {
      &quot;min&quot;: 0,
      &quot;max&quot;: 0
    },
    &quot;lowPriorityNodes&quot;: {
      &quot;min&quot;: 3,
      &quot;max&quot;: 3
    },
    &quot;autoscaleFormula&quot;: &quot;QUEUE&quot;
  },
  &quot;containerImage&quot;: &quot;rocker/tidyverse:latest&quot;,
  &quot;rPackages&quot;: {
    &quot;cran&quot;: [&quot;xgboost&quot;, &quot;caret&quot;],
    &quot;github&quot;: [],
    &quot;bioconductor&quot;: []
  },
  &quot;commandLine&quot;: []
}</code></pre>
<p>As you can see, Azure Batch uses a Docker image based on rocker/tidyverse and you can also specify additional packages to install. You can use different Docker images from Dockerhub or your own.</p>
<p>The following graphic from <a href="http://blog.revolutionanalytics.com/2017/11/doazureparallel-containers.html">RevolutionAnalytics.com</a> gives a nice overview about how that works:</p>
<div class="figure">
<img src="/img/doAzureParallel-Docker.png" alt="doAzureParallel" />
<p class="caption">doAzureParallel</p>
</div>
<p>The VMs we request have 2 cores each and 7GB of RAM and cost about € 0.12/hour as normal nodes and only € 0.02/hour in low-priority mode. Microsoft offers the following rule of thumb in choosing an appropriate VM:</p>
<ul>
<li>Av2 Series: economical, general purpose</li>
<li>F Series: compute intensive workloads</li>
<li>Dv2 Series: memory intensive workloads</li>
</ul>
<p>Now we can create our cluster (might take a few minutes) and register it as a foreach backend:</p>
<pre class="r"><code>cluster = makeCluster(&quot;cluster.json&quot;) 

## Re-establish connection to cluster (e.g. after R crashes):
# cluster = getCluster(&quot;myTestPool&quot;)

registerDoAzureParallel(cluster) 

workers = getDoParWorkers() </code></pre>
<p>The number of ‘execution workers’ = # nodes x maxTasksPerNode. When you look at message generated by <code>generateClusterConfig()</code> there is a note saying that to maximize all CPU corese, you shoult set maxTasksPerNote to 4x the number of cores of the VM you specified.</p>
<p>Let’s try our cluster by training a XGBoost model in a parallel fashion:</p>
</div>
<div id="install-packages" class="section level2">
<h2>Install packages</h2>
<p>As you can already see from the cluster.json file you can specifiy to install packages from:</p>
<ul>
<li>CRAN: <code>[&quot;cran_package_name_1&quot;, &quot;cran_package_name_2&quot;]</code></li>
<li>Github: <code>[&quot;github_username/github_package_1&quot;, &quot;another_github_username/another_github_package_2&quot;]</code></li>
<li>Bioconductor: <code>[&quot;some_bioconductor_package&quot;]</code></li>
</ul>
<p>You can also install packages from private github repos if you provide a Github authentification token in your credentials.json file.</p>
<p>You can also provide a custom docker image that already comes with all necessary packages.</p>
<p>It is also possible to install packages individually per foreach-loop like so:</p>
<pre class="r"><code>results = foreach(i = 1:number_of_iterations, 
                  .packages= c(&#39;package_1&#39;, &#39;package_2&#39;), 
                  github = c(&#39;github_username/github_package_1&#39;, &#39;github_username/github_package_2&#39;),
                  bioconductor = c(&#39;package_1&#39;, &#39;package_2&#39;)
                  ) %dopar% { ... }</code></pre>
<p>Currently, you cannot uninstall packages from a pool. Simply stop the cluster and start a new one.</p>
</div>
<div id="data-movement" class="section level2">
<h2>Data movement</h2>
<div id="export-local-r-session-default" class="section level3">
<h3>Export local R session (default)</h3>
<p>Since <code>doAzureParallel</code> is simply a <code>foreach</code> backend by default it exports the data you have in your local R session to all nodes. However, this implies that your data must fit into local memory and into memory of your workers.</p>
<pre class="r"><code>data = some_input_data

results = foreach(i = 1:10) %dopar% {
  some_algorithm(data)
}</code></pre>
</div>
<div id="export-data-using-iterators" class="section level3">
<h3>Export data using iterators</h3>
<p>Another alternative is to use iterators from the <code>iterators</code> package to split the data into parts and only distribute the relevant parts to each worker:</p>
<pre class="r"><code>## a dataset with a column called &#39;col_group&#39; that we can use to split the data 
## into parts that allow parallel processing
data = some_data_set

## generate (roughly) equal group sizes for workers
workers_splits = as.factor(as.numeric(as.factor(data[[col_group]]))%%workers)

## generate iterator and split data into groups for workers to process
iter_data = iterators::isplit(x = data, f = workers_splits) %dopar% {...}</code></pre>
</div>
<div id="pre-load-data-into-cluster" class="section level3">
<h3>Pre-load data into cluster</h3>
<p>You can pre-load data into your cluster when it is created using resource files. You can find more information on setting up resource files <a href="https://github.com/Azure/doAzureParallel/blob/master/docs/21-distributing-data.md">here</a></p>
<p>When you are done, it is important to stop the cluster. Otherwise, you will continue to incur charges.</p>
<pre class="r"><code>stopCluster(cluster)</code></pre>
</div>
</div>
<div id="saving-money" class="section level2">
<h2>Saving money</h2>
<p>Using low-priority VMs is an obvious way to save a significant chunk of money, but you can also use the ‘Autoscale’ feature to automatically change your cluster depending on your needs. <code>doAzureParallel</code> offers 4 autoscaling options out-of-the-box:</p>
<ul>
<li>QUEUE: scale pool based on amount of work in queue</li>
<li>WORKDAY: use MAX from Monday - Friday during business hours (8.00 - 18.00), else MIN</li>
<li>WEEKEND: use MAX on Saturday and Sunday, else MIN</li>
<li>MAX_CPU: Minimum average CPU usage &gt; 70% increases pool by 1.1X</li>
</ul>
<p>To use auto-scaling, you need to set minimum &lt; maximum number of nodes for both low prio and dedicated VMs.</p>
</div>
<div id="limits" class="section level2">
<h2>Limits</h2>
<p>Azure Batch can use up to 20 cores by default (irrespective of VM type), but you can request a higher limit by contacting Microsoft customer support.</p>
<p>You cannot run more than 20 foreach loops at a time, because each loop corresponds to 1 job and users are limited to 20 jobs in total.</p>
</div>
