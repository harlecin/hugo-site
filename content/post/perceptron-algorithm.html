---
title: "The Perceptron Algorithm"
author: "Christoph"
date: 2018-12-02T14:41:14-05:00
categories: ["Python"]
tags: ["Python", "Neural Nets"]
---



<p>In my blog post <a href="https://harlecin.netlify.com/post/neural-nets-from-scratch-in-r/">Neural Nets: From Linear Regression to Deep Nets</a> I talked about how a deep neural net is simply a sequence of simple building blocks of the form:</p>
<p><span class="math display">\[\sigma(\underbrace{w^T}_{weights}x + \overbrace{b}^{bias}) = a\]</span></p>
<p>and that a linear regression model is one of the most basic neural networks where the activation function <span class="math inline">\(\sigma\)</span> is the identity function:</p>
<ul>
<li>Linear Regression: <span class="math display">\[id(w^Tx + b) = a\]</span></li>
</ul>
<p>In this post I will show you how you can use a model called a <code>Perceptron</code> to classify points and thereby build the operators AND, OR and NOT. Furthermore, by combining multiple Perceptrons we can also model the XOR operator, which allows us to build more complex classifiers. A perceptron is defined as follows:</p>
<ul>
<li>Perceptron:
<span class="math display">\[
f(w^Tx + b) \text{ with } f(x) = \begin{cases}
1 &amp; \text{ if } w^Tx + b &gt; 0,\\
0 &amp; \text{ else}
\end{cases}
\]</span></li>
</ul>
<p>The activation function <span class="math inline">\(f\)</span> is also called <a href="https://en.wikipedia.org/wiki/Heaviside_step_function">Heaviside step function</a>. Note that this function is not differentiable at <span class="math inline">\(x = 0\)</span> and the derivative is 0 everywhere else. Since neural nets are trained using gradient descent this activation function is rarely used except in simple cases that can be calculated without gradient descent.</p>
<p>So the only difference between a Perceptron and linear regression is the activation function.</p>
<p>However, thinking about the properties of the Perceptron is still useful in my opinion as it provides interesting insights into more complex models.</p>
<p>The following examples are based on the excellent (and free!) Udacity course: ‘Introduction to Deep Learning with Pytorch’.</p>
<p>Let’s start with a few examples:</p>
<p>The AND operator is defined as follows:</p>
<table>
<thead>
<tr class="header">
<th>in</th>
<th>in</th>
<th>out</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="odd">
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr class="even">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>The OR operator is given by:</p>
<table>
<thead>
<tr class="header">
<th>in</th>
<th>in</th>
<th>out</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr class="odd">
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="even">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>How can we represent these operators using a <code>Perceptron</code>? Graphically, we want to find the following lines:
<img src="../../static/img/Perceptron-Operators.png" width="100%" /></p>
<p>Obviously there are multiple solutions that give the same answer. How can we find one of the solutions?</p>
<p>In <span class="math inline">\(\mathbb{R}^2\)</span> we can simply rewrite <span class="math inline">\(w_1x_1 + w_2x_2 + b = 0\)</span> as <span class="math inline">\(x_2 = -w_1/w_2x_1 - b/w_2\)</span> and solve the problem graphically. We could also solve the associated system of inequalities to get our solution.</p>
<p>But how can we find a ‘solution’ if it is not possible to classify all points correctly using a linear classifier? In math speak, how can we find a solution to an <a href="https://en.wikipedia.org/wiki/Overdetermined_system#Exact_solutions">overdetermined</a> and most likely inconsistent system?</p>
<p>This is were the perceptron trick comes in.</p>
<p>Suppose we misclasify a point. Say we predict label +1 for point <span class="math inline">\((0,1)\)</span>, but the point actually has label -1, how can we update the line?</p>
<p><img src="../../static/img/perceptron-algorithm.png" width="100%" /></p>
<p>Intuitively, the line should move closer to the misclassified point and ideally move past it. How can we do that? We can increase/decrease the bias and thereby affect the intercept of the line and we can increase/decrease the weights to change the slope of the line. The idea carries over to higher dimensonal spaces, with the only exception being that we don’t have a line separating the space, but a hyperplane.</p>
<p>We will now code the Perceptron algorithm to iteratively update the weights and the bias either until all points are classified correctly or up to a certain number of rounds.</p>
<pre><code>## Perceptron Algorithm
import numpy as np

np.random.seed(42)

X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([-1, 1, -1, 1])

W = np.array([1, 1])
bias = -0.5
yhat = np.sign(np.matmul(X,W) + bias)

num_epochs = 1
learn_rate = 0.01

while sum(abs(yhat-y)) &gt; 0 and num_epochs &lt;= 25:
    for row in range(0, len(X)):
        if y[row] * (np.matmul(X[row],W) + bias) &lt;= 0:
            ## if signs do not agree, we have a misclassification
            ## and a negative output
            W[0] += (y[row] * X[row][0])*learn_rate
            W[1] += (y[row] * X[row][1])*learn_rate
            bias += y[row]*learn_rate
            
            ## print results
            print(&quot;Weights: {}, Bias: {}, Epoch: {}&quot;.format(W, bias, num_epochs))
            
            ## update while conditions
            num_epochs += 1
            yhat = np.sign(np.matmul(X,W) + bias)

print(&quot;Prediction:\t {}\nActual:\t\t {}&quot;.format(yhat, y))</code></pre>
<pre><code>Weights: [0 1], Bias: -0.51, Epoch: 1
Prediction:  [-1  1 -1  1]
Actual:      [-1  1 -1  1]</code></pre>
<p>So what did we do?</p>
<p>Given a random starting hyperplane:</p>
<p><span class="math display">\[w_1x_1 +w_2x_2 +b=0\]</span></p>
<p>and a misclassified point <span class="math inline">\((c,d)\)</span> with label <span class="math inline">\(y \in \{-1,1\}\)</span> and learning rate <span class="math inline">\(l\)</span></p>
<p>we update the hyperplane as follows:
<span class="math display">\[
\begin{align*}
w_1&amp; +w_2 +b &amp;=0 \\
+y*l(c&amp;  +d  +b)&amp;=0 
\end{align*}
\]</span></p>
<p>So we are nudging the hyperplane closer to the misclassified point.</p>
<p>Rather simple, is it not?:)</p>
<p>However, note that the Perceptron algorithm can converge to any hyperplane that manages to classify most/all points correctly. But clearly, some hyperplanes are ‘better’ in the sense that they are more stable than others. The ‘perceptron of optimal stability’ is better known under the name ‘linear support vector machine’ (see <a href="http://www.lptms.u-psud.fr/membres/mezard/Pdf/87_MK_JPA.pdf">Krauth and Mezard, 1987</a>)</p>
<p>I hope you found the post interesting!</p>
<div id="references" class="section level2">
<h2>References:</h2>
<p>The Perceptron algorithm above is based on the Pseudo Code from:</p>
<ul>
<li><a href="https://jeremykun.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/">The Perceptron, and All the Things it Can’t Perceive</a></li>
</ul>
<p>with small adaptions.</p>
</div>
